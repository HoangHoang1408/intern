{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train CBOW with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "device = 'cpu'\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "sns.set()\n",
    "rcParams['figure.figsize'] = (20,10)\n",
    "pd.options.display.max_columns = None\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = {}\n",
    "with open('../data/all_simplified2.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        for word in line.split():\n",
    "            word_count[word] = word_count.get(word, 0) + 1\n",
    "          \n",
    "word_count = sorted(word_count.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_df = pd.DataFrame(word_count, columns=['word', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5946"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = word_count_df[word_count_df['count'] >= 50]['word'].values\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {word: idx for idx, word in enumerate(vocab, 1)}\n",
    "word2idx['<unk>'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5947"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "895bbe13a37348d9a85dc4b9535eeed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function tqdm.__del__ at 0x117ff03a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tqdm/std.py\", line 1161, in __del__\n",
      "    def __del__(self):\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "window = 4 # 5 words on each side\n",
    "main_dataset = []\n",
    "\n",
    "with open('../data/all_simplified2.txt', 'r') as f:\n",
    "    for line in tqdm(f.readlines()):\n",
    "        words = line.strip().split(' ')\n",
    "        for idx, word in enumerate(words):\n",
    "            if word not in word2idx:\n",
    "                continue\n",
    "            context_indices = [word2idx.get(words[t],0) for t in range(idx - window, idx + window + 1) if t >= 0 and t < len(words) and t != idx]\n",
    "            context_indices = context_indices + [0] * (2 * window - len(context_indices))\n",
    "            main_dataset.append((word2idx[word], context_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4652283"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(main_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3145, [22, 0, 0, 158, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWDataset(Dataset):\n",
    "    def __init__(self, dataset, cur_set='train'):\n",
    "        self.cur_set = cur_set\n",
    "        train_, test = train_test_split(dataset, test_size=0.9, random_state=42)\n",
    "        train, val = train_test_split(train_, test_size=0.1666, random_state=42)\n",
    "        self.lookup = {'train': train, 'val': val, 'test': test}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lookup[self.cur_set])\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.lookup[self.cur_set][idx]\n",
    "        context = torch.tensor(row[1], dtype=torch.long).to(device)\n",
    "        target = torch.tensor(row[0], dtype=torch.long).to(device)\n",
    "        return context, target\n",
    "\n",
    "    def get_dl(self, batch_size, shuffle=True, cur_set='train', drop_last=True):\n",
    "        self.cur_set = cur_set\n",
    "        return DataLoader(self, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        x = F.dropout(x, 0.2)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CBOWModel(len(word2idx), 100).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "ds = CBOWDataset(main_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4c1914507a147d5821e71d84a5e671a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10005 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "234d5ea8dac84d5287fe982204816f67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 6.5373, Train Acc: 0.1137, Val Loss: 6.2692, Val Acc: 0.1178\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a11dca3572c4215a45882e1bfeacadb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10005 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/phamhoang1408/Desktop/Learn 2/2 Word2Vec/code/3.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/phamhoang1408/Desktop/Learn%202/2%20Word2Vec/code/3.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(out, target)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/phamhoang1408/Desktop/Learn%202/2%20Word2Vec/code/3.ipynb#X20sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/phamhoang1408/Desktop/Learn%202/2%20Word2Vec/code/3.ipynb#X20sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/phamhoang1408/Desktop/Learn%202/2%20Word2Vec/code/3.ipynb#X20sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m train_losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/phamhoang1408/Desktop/Learn%202/2%20Word2Vec/code/3.ipynb#X20sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m train_accs\u001b[39m.\u001b[39mappend((out\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m==\u001b[39m target)\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mmean()\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/optim/optimizer.py:139\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m obj, \u001b[39m*\u001b[39m_ \u001b[39m=\u001b[39m args\n\u001b[1;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m--> 139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m    140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/autograd/profiler.py:488\u001b[0m, in \u001b[0;36mrecord_function.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__enter__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 488\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mprofiler\u001b[39m.\u001b[39;49m_record_function_enter(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs)\n\u001b[1;32m    489\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/_ops.py:442\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    438\u001b[0m     \u001b[39m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[1;32m    439\u001b[0m     \u001b[39m# is still callable from JIT\u001b[39;00m\n\u001b[1;32m    440\u001b[0m     \u001b[39m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[1;32m    441\u001b[0m     \u001b[39m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_op(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs \u001b[39mor\u001b[39;49;00m {})\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "def train_model(epochs=1, batch_size=128):\n",
    "    set_seed(42)\n",
    "    for epoch in range(epochs):\n",
    "        train_losses, train_accs = [], []\n",
    "        dl = ds.get_dl(batch_size)\n",
    "        model.train()\n",
    "        for context, target in tqdm(dl):\n",
    "            context = context.to(device)\n",
    "            target = target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(context)\n",
    "            loss = loss_fn(out, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "            train_accs.append((out.argmax(dim=1) == target).float().mean().item())\n",
    "        \n",
    "        val_losses, val_accs = [], []\n",
    "        dl = ds.get_dl(batch_size, cur_set='val')\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for context, target in tqdm(dl):\n",
    "                context = context.to(device)\n",
    "                target = target.to(device)\n",
    "                out = model(context)\n",
    "                loss = loss_fn(out, target)\n",
    "                val_losses.append(loss.item())\n",
    "                val_accs.append((out.argmax(dim=1) == target).float().mean().item())\n",
    "\n",
    "        print(f'Epoch: {epoch + 1}, Train Loss: {np.mean(train_losses):.4f}, Train Acc: {np.mean(train_accs):.4f}, Val Loss: {np.mean(val_losses):.4f}, Val Acc: {np.mean(val_accs):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on test set\n",
    "test_losses, test_accs = [], []\n",
    "dl = ds.get_dl(batch_size, cur_set='test')\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    losses, accs = [], []\n",
    "    for context, target in tqdm(dl):\n",
    "        context = context.to(device)\n",
    "        target = target.to(device)\n",
    "        out = model(context)\n",
    "        loss = loss_fn(out, target)\n",
    "        test_losses.append(loss.item())\n",
    "        test_accs.append((out.argmax(dim=1) == target).float().mean().item())\n",
    "\n",
    "print(f'Test Loss: {np.mean(test_losses):.4f}, Test Acc: {np.mean(test_accs):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model state dict\n",
    "path = '../models/cbow_model.pth'\n",
    "torch.save(model.state_dict(), path)\n",
    "\n",
    "# save word2idx\n",
    "path = '../models/cbow_word2idx.json'\n",
    "import json\n",
    "json.dump(word2idx, open(path, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annoy import AnnoyIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "\n",
    "class WordPlay:\n",
    "    def __init__(self, word2idx, embeddings):\n",
    "        self.word2idx = word2idx\n",
    "        self.idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "        self.embeddings = embeddings\n",
    "        self.annoy_index = AnnoyIndex(embeddings.shape[1], 'angular')\n",
    "        for idx in range(embeddings.shape[0]):\n",
    "            self.annoy_index.add_item(idx, embeddings[idx])\n",
    "        self.annoy_index.build(10)\n",
    "\n",
    "    def get_similar_words(self, word, n=10):\n",
    "        idx = self.word2idx.get(word, 0)\n",
    "        if idx == 0:\n",
    "            return []\n",
    "        similar_indices, similar_distances = self.annoy_index.get_nns_by_vector(idx, n, include_distances=True)\n",
    "        return [(self.idx2word[idx], dist) for idx, dist in zip(similar_indices, similar_distances) if idx in self.idx2word]\n",
    "    \n",
    "    def get_analogy(self, word1, word2, word3, n=10):\n",
    "        idx1 = self.word2idx.get(word1, 0)\n",
    "        idx2 = self.word2idx.get(word2, 0)\n",
    "        idx3 = self.word2idx.get(word3, 0)\n",
    "        if idx1 == 0 or idx2 == 0 or idx3 == 0:\n",
    "            return []\n",
    "        vec = self.embeddings[idx2] - self.embeddings[idx1] + self.embeddings[idx3]\n",
    "        similar_indices, similar_distances = self.annoy_index.get_nns_by_vector(vec, n, include_distances=True)\n",
    "        return [(self.idx2word[idx], dist) for idx, dist in zip(similar_indices, similar_distances) if idx in self.idx2word]\n",
    "    \n",
    "    def get_random_similar_words(self, word, n=10):\n",
    "        random_words = np.random.choice(list(self.word2idx.keys()), n)\n",
    "        for word in random_words:\n",
    "            pprint(self.get_similar_words(word))\n",
    "            print('-'*30)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word2idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/phamhoang1408/Desktop/Learn 2/2 Word2Vec/code/3.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/phamhoang1408/Desktop/Learn%202/2%20Word2Vec/code/3.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m play \u001b[39m=\u001b[39m WordPlay(word2idx, model\u001b[39m.\u001b[39membeddings\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word2idx' is not defined"
     ]
    }
   ],
   "source": [
    "play = WordPlay(word2idx, model.embeddings.weight.detach().cpu().numpy())\n",
    "play.get_similar_words('vua')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CBOWModel(len(word2idx), 100)\n",
    "model.load_state_dict(torch.load('../models/cbow_model.pth'))\n",
    "\n",
    "word2idx = json.load(open('../models/cbow_word2idx.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chose random words in the vocabulary\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "random_words = np.random.choice(list(word2idx.keys()), 10)\n",
    "for word in random_words:\n",
    "    pprint(play.get_similar_words(word))\n",
    "    print('-'*30)\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
