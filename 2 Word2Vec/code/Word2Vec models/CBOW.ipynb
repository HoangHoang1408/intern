{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train CBOW with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "device = 'cpu'\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "sns.set()\n",
    "rcParams['figure.figsize'] = (20,10)\n",
    "pd.options.display.max_columns = None\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = {}\n",
    "with open('../data/all_simplified2.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        for word in line.split():\n",
    "            word_count[word] = word_count.get(word, 0) + 1\n",
    "          \n",
    "word_count = sorted(word_count.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_df = pd.DataFrame(word_count, columns=['word', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = word_count_df[word_count_df['count'] >= 50]['word'].values\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {word: idx for idx, word in enumerate(vocab, 1)}\n",
    "word2idx['<unk>'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 4 # 5 words on each side\n",
    "main_dataset = []\n",
    "\n",
    "with open('../data/all_simplified2.txt', 'r') as f:\n",
    "    for line in tqdm(f.readlines()):\n",
    "        words = line.strip().split(' ')\n",
    "        for idx, word in enumerate(words):\n",
    "            if word not in word2idx:\n",
    "                continue\n",
    "            context_indices = [word2idx.get(words[t],0) for t in range(idx - window, idx + window + 1) if t >= 0 and t < len(words) and t != idx]\n",
    "            context_indices = context_indices + [0] * (2 * window - len(context_indices))\n",
    "            main_dataset.append((word2idx[word], context_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(main_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWDataset(Dataset):\n",
    "    def __init__(self, dataset, cur_set='train'):\n",
    "        self.cur_set = cur_set\n",
    "        train_, test = train_test_split(dataset, test_size=0.9, random_state=42)\n",
    "        train, val = train_test_split(train_, test_size=0.1666, random_state=42)\n",
    "        self.lookup = {'train': train, 'val': val, 'test': test}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lookup[self.cur_set])\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.lookup[self.cur_set][idx]\n",
    "        context = torch.tensor(row[1], dtype=torch.long).to(device)\n",
    "        target = torch.tensor(row[0], dtype=torch.long).to(device)\n",
    "        return context, target\n",
    "\n",
    "    def get_dl(self, batch_size, shuffle=True, cur_set='train', drop_last=True):\n",
    "        self.cur_set = cur_set\n",
    "        return DataLoader(self, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        x = F.dropout(x, 0.2)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CBOWModel(len(word2idx), 100).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "ds = CBOWDataset(main_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "def train_model(epochs=1, batch_size=128):\n",
    "    set_seed(42)\n",
    "    for epoch in range(epochs):\n",
    "        train_losses, train_accs = [], []\n",
    "        dl = ds.get_dl(batch_size)\n",
    "        model.train()\n",
    "        for context, target in tqdm(dl):\n",
    "            context = context.to(device)\n",
    "            target = target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(context)\n",
    "            loss = loss_fn(out, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "            train_accs.append((out.argmax(dim=1) == target).float().mean().item())\n",
    "        \n",
    "        val_losses, val_accs = [], []\n",
    "        dl = ds.get_dl(batch_size, cur_set='val')\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for context, target in tqdm(dl):\n",
    "                context = context.to(device)\n",
    "                target = target.to(device)\n",
    "                out = model(context)\n",
    "                loss = loss_fn(out, target)\n",
    "                val_losses.append(loss.item())\n",
    "                val_accs.append((out.argmax(dim=1) == target).float().mean().item())\n",
    "\n",
    "        print(f'Epoch: {epoch + 1}, Train Loss: {np.mean(train_losses):.4f}, Train Acc: {np.mean(train_accs):.4f}, Val Loss: {np.mean(val_losses):.4f}, Val Acc: {np.mean(val_accs):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on test set\n",
    "test_losses, test_accs = [], []\n",
    "dl = ds.get_dl(batch_size, cur_set='test')\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    losses, accs = [], []\n",
    "    for context, target in tqdm(dl):\n",
    "        context = context.to(device)\n",
    "        target = target.to(device)\n",
    "        out = model(context)\n",
    "        loss = loss_fn(out, target)\n",
    "        test_losses.append(loss.item())\n",
    "        test_accs.append((out.argmax(dim=1) == target).float().mean().item())\n",
    "\n",
    "print(f'Test Loss: {np.mean(test_losses):.4f}, Test Acc: {np.mean(test_accs):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model state dict\n",
    "path = '../models/cbow_model.pth'\n",
    "torch.save(model.state_dict(), path)\n",
    "\n",
    "# save word2idx\n",
    "path = '../models/cbow_word2idx.json'\n",
    "import json\n",
    "json.dump(word2idx, open(path, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annoy import AnnoyIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "\n",
    "class WordPlay:\n",
    "    def __init__(self, word2idx, embeddings):\n",
    "        self.word2idx = word2idx\n",
    "        self.idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "        self.embeddings = embeddings\n",
    "        self.annoy_index = AnnoyIndex(embeddings.shape[1], 'angular')\n",
    "        for idx in range(embeddings.shape[0]):\n",
    "            self.annoy_index.add_item(idx, embeddings[idx])\n",
    "        self.annoy_index.build(10)\n",
    "\n",
    "    def get_similar_words(self, word, n=10):\n",
    "        idx = self.word2idx.get(word, 0)\n",
    "        if idx == 0:\n",
    "            return []\n",
    "        similar_indices, similar_distances = self.annoy_index.get_nns_by_vector(idx, n, include_distances=True)\n",
    "        return [(self.idx2word[idx], dist) for idx, dist in zip(similar_indices, similar_distances) if idx in self.idx2word]\n",
    "    \n",
    "    def get_analogy(self, word1, word2, word3, n=10):\n",
    "        idx1 = self.word2idx.get(word1, 0)\n",
    "        idx2 = self.word2idx.get(word2, 0)\n",
    "        idx3 = self.word2idx.get(word3, 0)\n",
    "        if idx1 == 0 or idx2 == 0 or idx3 == 0:\n",
    "            return []\n",
    "        vec = self.embeddings[idx2] - self.embeddings[idx1] + self.embeddings[idx3]\n",
    "        similar_indices, similar_distances = self.annoy_index.get_nns_by_vector(vec, n, include_distances=True)\n",
    "        return [(self.idx2word[idx], dist) for idx, dist in zip(similar_indices, similar_distances) if idx in self.idx2word]\n",
    "    \n",
    "    def get_random_similar_words(self, word, n=10):\n",
    "        random_words = np.random.choice(list(self.word2idx.keys()), n)\n",
    "        for word in random_words:\n",
    "            pprint(self.get_similar_words(word))\n",
    "            print('-'*30)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play = WordPlay(word2idx, model.embeddings.weight.detach().cpu().numpy())\n",
    "play.get_similar_words('vua')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CBOWModel(len(word2idx), 100)\n",
    "model.load_state_dict(torch.load('../models/cbow_model.pth'))\n",
    "\n",
    "word2idx = json.load(open('../models/cbow_word2idx.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chose random words in the vocabulary\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "random_words = np.random.choice(list(word2idx.keys()), 10)\n",
    "for word in random_words:\n",
    "    pprint(play.get_similar_words(word))\n",
    "    print('-'*30)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
