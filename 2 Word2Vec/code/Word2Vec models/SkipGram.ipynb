{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "sns.set()\n",
    "rcParams['figure.figsize'] = (20,10)\n",
    "pd.options.display.max_columns = None\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import re\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from annoy import AnnoyIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r'[^\\s\\wáàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệóòỏõọôốồổỗộơớờởỡợíìỉĩịúùủũụưứừửữựýỳỷỹỵđ_]','',text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = {}\n",
    "with open('./data/all_simplified2.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        for word in preprocess(line).split():\n",
    "            if word in ['punct','number']: continue\n",
    "            word_count[word] = word_count.get(word, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_df = pd.DataFrame(word_count.items(), columns=['word', 'count'])\n",
    "word_count_df = word_count_df.sort_values('count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12199"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = word_count_df[word_count_df['count'] >= 15]['word'].values\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {word: idx for idx, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "window = 2\n",
    "with open('./data/all_simplified2.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        words = preprocess(line).split()\n",
    "        for i, word in enumerate(words):\n",
    "            if word not in word2idx: continue\n",
    "            for j in range(i-window, i+window+1):\n",
    "                if j < 0 or j >= len(words) or j == i: continue\n",
    "                if words[j] not in word2idx: continue\n",
    "                dataset.append((word2idx[word], word2idx[words[j]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramDataset(Dataset):\n",
    "    def __init__(self, dataset, cur_set='train'):\n",
    "        self.cur_set = cur_set\n",
    "        train_, test = train_test_split(dataset, test_size=0.1, random_state=42)\n",
    "        train, valid = train_test_split(train_, test_size=0.1666, random_state=42)\n",
    "        self.lookup = {\n",
    "            'train': train.copy(),\n",
    "            'valid': valid.copy(),\n",
    "            'test': test.copy()\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lookup[self.cur_set])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.lookup[self.cur_set][idx]\n",
    "        return torch.tensor(x).to(device), torch.tensor(y).to(device)\n",
    "    \n",
    "    def get_dl(self, cur_set='train', batch_size=32, shuffle=True, drop_last=True):\n",
    "        self.cur_set = cur_set\n",
    "        return DataLoader(self, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.linear = nn.Linear(embedding_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SkipGramDataset(dataset)\n",
    "model = SkipGramModel(len(word2idx), 100).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset, loss_fn, optimizer, epochs=5):\n",
    "    for epoch in range(epochs):\n",
    "        losses, accs = [], []\n",
    "        model.train()\n",
    "        for x, y in tqdm(dataset.get_dl('train', batch_size=32, shuffle=True, drop_last=True)):\n",
    "            y_hat = model(x)\n",
    "            loss = loss_fn(y_hat, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "            accs.append((y_hat.argmax(1) == y).float().mean().item())\n",
    "\n",
    "        val_losses, val_accs = [], []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for x, y in tqdm(dataset.get_dl('valid', batch_size=32, shuffle=True, drop_last=True)):\n",
    "                y_hat = model(x)\n",
    "                loss = loss_fn(y_hat, y)\n",
    "                val_losses.append(loss.item())\n",
    "                val_accs.append((y_hat.argmax(1) == y).float().mean().item())\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{epochs}: Train loss: {np.mean(losses):.4f}, Train acc: {np.mean(accs):.4f}, Val loss: {np.mean(val_losses):.4f}, Val acc: {np.mean(val_accs):.4f}')\n",
    "\n",
    "def test(model, dataset, loss_fn):\n",
    "    losses, accs = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in tqdm(dataset.get_dl('test', batch_size=32, shuffle=True, drop_last=True)):\n",
    "            y_hat = model(x)\n",
    "            loss = loss_fn(y_hat, y)\n",
    "            losses.append(loss.item())\n",
    "            accs.append((y_hat.argmax(1) == y).float().mean().item())\n",
    "    print(f'Test loss: {np.mean(losses):.4f}, Test acc: {np.mean(accs):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3aaaf02e3704356851bbbbfa0aafffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/314380 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/phamhoang1408/Desktop/Learn/Intern/2 Word2Vec/code/Word2Vec models/SkipGram.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/phamhoang1408/Desktop/Learn/Intern/2%20Word2Vec/code/Word2Vec%20models/SkipGram.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train(model, dataset, loss_fn, optimizer, epochs\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\n",
      "\u001b[1;32m/Users/phamhoang1408/Desktop/Learn/Intern/2 Word2Vec/code/Word2Vec models/SkipGram.ipynb Cell 13\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataset, loss_fn, optimizer, epochs)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/phamhoang1408/Desktop/Learn/Intern/2%20Word2Vec/code/Word2Vec%20models/SkipGram.ipynb#X21sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/phamhoang1408/Desktop/Learn/Intern/2%20Word2Vec/code/Word2Vec%20models/SkipGram.ipynb#X21sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/phamhoang1408/Desktop/Learn/Intern/2%20Word2Vec/code/Word2Vec%20models/SkipGram.ipynb#X21sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39;49mitem())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/phamhoang1408/Desktop/Learn/Intern/2%20Word2Vec/code/Word2Vec%20models/SkipGram.ipynb#X21sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     accs\u001b[39m.\u001b[39mappend((y_hat\u001b[39m.\u001b[39margmax(\u001b[39m1\u001b[39m) \u001b[39m==\u001b[39m y)\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mmean()\u001b[39m.\u001b[39mitem())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/phamhoang1408/Desktop/Learn/Intern/2%20Word2Vec/code/Word2Vec%20models/SkipGram.ipynb#X21sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m val_losses, val_accs \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, dataset, loss_fn, optimizer, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model, dataset, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model.state_dict(), './model/skipgram.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "class WordPlay:\n",
    "    def __init__(self, word2idx, embeddings):\n",
    "        self.word2idx = word2idx\n",
    "        self.idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "        self.embeddings = embeddings\n",
    "        self.annoy_index = AnnoyIndex(embeddings.shape[1], 'angular')\n",
    "        for idx in range(embeddings.shape[0]):\n",
    "            self.annoy_index.add_item(idx, embeddings[idx])\n",
    "        self.annoy_index.build(10)\n",
    "\n",
    "    def get_similar_words(self, word, n=10):\n",
    "        idx = self.word2idx.get(word, 0)\n",
    "        if idx == 0:\n",
    "            return []\n",
    "        similar_indices, similar_distances = self.annoy_index.get_nns_by_item(idx, n, include_distances=True)\n",
    "        return [(self.idx2word[idx], dist) for idx, dist in zip(similar_indices, similar_distances) if idx in self.idx2word]\n",
    "    \n",
    "    def get_analogy(self, word1, word2, word3, n=10):\n",
    "        idx1 = self.word2idx.get(word1, 0)\n",
    "        idx2 = self.word2idx.get(word2, 0)\n",
    "        idx3 = self.word2idx.get(word3, 0)\n",
    "        if idx1 == 0 or idx2 == 0 or idx3 == 0:\n",
    "            return []\n",
    "        vec = self.embeddings[idx2] - self.embeddings[idx1] + self.embeddings[idx3]\n",
    "        similar_indices, similar_distances = self.annoy_index.get_nns_by_vector(vec, n, include_distances=True)\n",
    "        return [(self.idx2word[idx], dist) for idx, dist in zip(similar_indices, similar_distances) if idx in self.idx2word]\n",
    "    \n",
    "    def get_random_similar_words(self, n=10):\n",
    "        random_words = np.random.choice(list(self.word2idx.keys()), n)\n",
    "        for word in random_words:\n",
    "            pprint(self.get_similar_words(word))\n",
    "            print('-'*30)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_play = WordPlay(word2idx, model.embedding.weight.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_play.get_random_similar_words(n=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
